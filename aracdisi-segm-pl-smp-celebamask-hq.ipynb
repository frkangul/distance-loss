{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# YAPILACAKLAR:\n* Check min(boundary iou, iou) loss function implementation\n* test time augmentation (tta)\n* reduce rl on plateo: load best ckpt model when reducing learning rate. Callback bunu yapıyor. Fakat best ckpt model yüklenince patience/learning rate de eski haline geri geliyor.\n* combine reduce rl on plateo and CosineAnnealingLR. Or change factor with cosineannealing.","metadata":{}},{"cell_type":"markdown","source":"# INSTALL EXTERNAL LIBS","metadata":{}},{"cell_type":"code","source":"!pip install pytorch-lightning==1.7.7\n!pip install wandb==0.13.10\n!pip install pycocotools==2.0.6\n!pip install python-box==7.0.0\n!pip install segmentation-models-pytorch==0.3.2\n!pip install albumentations==1.3.0","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:17:52.921703Z","iopub.execute_input":"2023-07-24T13:17:52.922353Z","iopub.status.idle":"2023-07-24T13:19:49.087173Z","shell.execute_reply.started":"2023-07-24T13:17:52.922242Z","shell.execute_reply":"2023-07-24T13:19:49.085872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DEFINE SOME PARAMS","metadata":{}},{"cell_type":"code","source":"cfg = {\n    \"max_epoch\": 60,\n    \"max_time\": \"00:11:55:00\", \n    \"distance_transform_loss\": False,\n    \"wandb\": {\n        \"exp_name\": \"dice\",\n        \"proj_name\": \"foreground-car-segm\",\n    },\n    \"model_ckpt_motior\": \"val_per_image_bIoU\",\n    \"data_name\": \"CelebAMask-HQ-eyes\",\n    \"output_class_num\": 1,\n    \"mode\": \"binary\",\n    \"data_dir\": \"/kaggle/input/celebamask-hq-v3-eyes/CelebAMask-HQ-v3-eyes/\",  \n    \"trainer\": {\n        \"accelerator\": \"auto\", # gpu, tpu, cpu, auto\n        \"device_num\": 2, # Number of gpu, check if T4 vs P100\n        \"precision\": 16, # Double precision (64), full precision (32), half precision (16) or bfloat16 precision (bf16)\n        \"ckpt_path4resume\": \"/kaggle/input/epoch42-unet/epoch42-Unet-resnet34-lr0.0001-hight512-width512-dodspyne.ckpt\",\n    },\n    \"transform\": {\n        \"image_resize_h\": 256, # 576\n        \"image_resize_w\": 256, # 800\n    },\n    \"model\": {\n        \"model_name\": \"Unet\", # \"DeepLabV3Plus\", \"UnetPlusPlus\", \"Unet\"\n        \"encoder_name\": \"resnet50\", # \"efficientnet-b5\",\"resnet34\", \"mit_b1\"       \n    },\n    \"optimizer\": {\n        \"lr\": 0.0003, # 0.001, 0.0003\n        \"reduce_rl_on\": False,\n    },   \n    \"patience\": 20, # for validation loss\n    \"train_dl\": {\n        \"batch_size\": 32,\n    },\n    \"val_dl\": {\n        \"batch_size\": 32,\n    },\n    \"test_dl\": {\n        \"batch_size\": 32,\n    },\n    \"SEED\": 42,\n    \"vis_img_num\": 8,\n    \"vis_val_batch_id\": 5,\n    \"ckpt_save_dir\": '/kaggle/working/logs/lightning_logs/checkpoints/'\n}","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:19:49.090909Z","iopub.execute_input":"2023-07-24T13:19:49.091609Z","iopub.status.idle":"2023-07-24T13:19:49.101747Z","shell.execute_reply.started":"2023-07-24T13:19:49.091573Z","shell.execute_reply":"2023-07-24T13:19:49.100697Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# IMPORT SECRETS","metadata":{}},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nsecret_label = \"wandb-api-key\"\nwandb_secret_value = UserSecretsClient().get_secret(secret_label)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:19:49.102968Z","iopub.execute_input":"2023-07-24T13:19:49.103406Z","iopub.status.idle":"2023-07-24T13:19:49.260531Z","shell.execute_reply.started":"2023-07-24T13:19:49.103368Z","shell.execute_reply":"2023-07-24T13:19:49.259630Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# IMPORT LIBS","metadata":{}},{"cell_type":"markdown","source":"### Install Torch-XLA (PyTorch with Accelerated Linear Algebra (XLA) support) for TPU\n#### https://www.kaggle.com/docs/tpu#tpu9\nAslında TPU VM v3-8 içerisinde torch ve torch x-la yüklü geliyor, bundan dolayı x-la yüklemeye gerek yok. Fakat TPU, pl ile stabil çalışmıyor.","metadata":{}},{"cell_type":"code","source":"!python -V","metadata":{"execution":{"iopub.status.busy":"2023-07-21T12:21:52.449191Z","iopub.execute_input":"2023-07-21T12:21:52.449601Z","iopub.status.idle":"2023-07-21T12:21:53.427525Z","shell.execute_reply.started":"2023-07-21T12:21:52.449567Z","shell.execute_reply":"2023-07-21T12:21:53.426136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!nvcc --version","metadata":{"execution":{"iopub.status.busy":"2023-07-21T12:21:53.429702Z","iopub.execute_input":"2023-07-21T12:21:53.430031Z","iopub.status.idle":"2023-07-21T12:21:54.402990Z","shell.execute_reply.started":"2023-07-21T12:21:53.430000Z","shell.execute_reply":"2023-07-21T12:21:54.401687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip show torch","metadata":{"execution":{"iopub.status.busy":"2023-07-21T12:21:54.404977Z","iopub.execute_input":"2023-07-21T12:21:54.405348Z","iopub.status.idle":"2023-07-21T12:22:05.956816Z","shell.execute_reply.started":"2023-07-21T12:21:54.405313Z","shell.execute_reply":"2023-07-21T12:22:05.955546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip show torchvision","metadata":{"execution":{"iopub.status.busy":"2023-07-21T12:22:05.961471Z","iopub.execute_input":"2023-07-21T12:22:05.965020Z","iopub.status.idle":"2023-07-21T12:22:16.972726Z","shell.execute_reply.started":"2023-07-21T12:22:05.964985Z","shell.execute_reply":"2023-07-21T12:22:16.971423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip show torchaudio","metadata":{"execution":{"iopub.status.busy":"2023-07-21T12:22:16.974975Z","iopub.execute_input":"2023-07-21T12:22:16.975720Z","iopub.status.idle":"2023-07-21T12:22:27.961752Z","shell.execute_reply.started":"2023-07-21T12:22:16.975676Z","shell.execute_reply":"2023-07-21T12:22:27.960447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip show torch-xla","metadata":{"execution":{"iopub.status.busy":"2023-07-21T12:22:27.965159Z","iopub.execute_input":"2023-07-21T12:22:27.965601Z","iopub.status.idle":"2023-07-21T12:22:29.730106Z","shell.execute_reply.started":"2023-07-21T12:22:27.965553Z","shell.execute_reply":"2023-07-21T12:22:29.728843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://github.com/pytorch/xla#running-pytorchxla-on-cloud-tpu-and-gpu\n# !pip install cloud-tpu-client==0.10 torch==1.13.0 https://storage.googleapis.com/tpu-pytorch/wheels/tpuvm/torch_xla-1.13-cp38-cp38-linux_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2023-07-21T12:22:29.733593Z","iopub.execute_input":"2023-07-21T12:22:29.733995Z","iopub.status.idle":"2023-07-21T12:22:29.741217Z","shell.execute_reply.started":"2023-07-21T12:22:29.733960Z","shell.execute_reply":"2023-07-21T12:22:29.740231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# COPIED DIRECTLY FROM LATEST PYTORCH SOURCE\nimport math\nfrom torch.utils.data import Dataset\nfrom torch import Generator\nfrom torch import default_generator, randperm\nfrom torch._utils import _accumulate\nfrom torch.utils.data.dataset import Subset\nfrom typing import (\n    Generic,\n    Iterable,\n    Iterator,\n    List,\n    Optional,\n    Sequence,\n    Tuple,\n    TypeVar,\n    Union\n)\n\nT = TypeVar('T')\n\ndef random_split(dataset: Dataset[T], lengths: Sequence[Union[int, float]],\n                 generator: Optional[Generator] = default_generator) -> List[Subset[T]]:\n    r\"\"\"\n    Randomly split a dataset into non-overlapping new datasets of given lengths.\n\n    If a list of fractions that sum up to 1 is given,\n    the lengths will be computed automatically as\n    floor(frac * len(dataset)) for each fraction provided.\n\n    After computing the lengths, if there are any remainders, 1 count will be\n    distributed in round-robin fashion to the lengths\n    until there are no remainders left.\n\n    Optionally fix the generator for reproducible results, e.g.:\n\n    >>> random_split(range(10), [3, 7], generator=torch.Generator().manual_seed(42))\n    >>> random_split(range(30), [0.3, 0.3, 0.4], generator=torch.Generator(\n    ...   ).manual_seed(42))\n\n    Args:\n        dataset (Dataset): Dataset to be split\n        lengths (sequence): lengths or fractions of splits to be produced\n        generator (Generator): Generator used for the random permutation.\n    \"\"\"\n    if math.isclose(sum(lengths), 1) and sum(lengths) <= 1:\n        subset_lengths: List[int] = []\n        for i, frac in enumerate(lengths):\n            if frac < 0 or frac > 1:\n                raise ValueError(f\"Fraction at index {i} is not between 0 and 1\")\n            n_items_in_split = int(\n                math.floor(len(dataset) * frac)  # type: ignore[arg-type]\n            )\n            subset_lengths.append(n_items_in_split)\n        remainder = len(dataset) - sum(subset_lengths)  # type: ignore[arg-type]\n        # add 1 to all the lengths in round-robin fashion until the remainder is 0\n        for i in range(remainder):\n            idx_to_add_at = i % len(subset_lengths)\n            subset_lengths[idx_to_add_at] += 1\n        lengths = subset_lengths\n        for i, length in enumerate(lengths):\n            if length == 0:\n                warnings.warn(f\"Length of split at index {i} is 0. \"\n                              f\"This might result in an empty dataset.\")\n\n    # Cannot verify that dataset is Sized\n    if sum(lengths) != len(dataset):    # type: ignore[arg-type]\n        raise ValueError(\"Sum of input lengths does not equal the length of the input dataset!\")\n\n    indices = randperm(sum(lengths), generator=generator).tolist()  # type: ignore[call-overload]\n    return [Subset(dataset, indices[offset - length : offset]) for offset, length in zip(_accumulate(lengths), lengths)]","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:19:49.263791Z","iopub.execute_input":"2023-07-24T13:19:49.265285Z","iopub.status.idle":"2023-07-24T13:19:51.030689Z","shell.execute_reply.started":"2023-07-24T13:19:49.265246Z","shell.execute_reply":"2023-07-24T13:19:51.029622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.nn import BCEWithLogitsLoss\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.datasets import CocoDetection, VisionDataset\nfrom torchvision.utils import draw_segmentation_masks\nimport torchvision.transforms.functional as visF\n# from torchmetrics import Dice # https://github.com/Lightning-AI/metrics/tree/master/src/torchmetrics\n\nimport os, random\nfrom typing import Any, Callable, List, Optional, Tuple\nimport numpy as np\nimport matplotlib.pyplot as plt \nfrom PIL import Image\nimport pandas as pd\nimport cv2\nfrom pathlib import Path\nfrom scipy import ndimage\n\nfrom pprint import pprint\nfrom box import Box\nfrom pycocotools.coco import COCO\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks.progress import TQDMProgressBar\nfrom pytorch_lightning.loggers import CSVLogger, WandbLogger\nfrom pytorch_lightning.utilities.model_summary import ModelSummary\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor, Callback\n\nimport segmentation_models_pytorch as smp\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nimport wandb\nwandb.login(key=wandb_secret_value)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:19:51.032283Z","iopub.execute_input":"2023-07-24T13:19:51.032958Z","iopub.status.idle":"2023-07-24T13:19:59.592374Z","shell.execute_reply.started":"2023-07-24T13:19:51.032921Z","shell.execute_reply":"2023-07-24T13:19:59.591042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\") # category=DeprecationWarning","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:19:59.594490Z","iopub.execute_input":"2023-07-24T13:19:59.595117Z","iopub.status.idle":"2023-07-24T13:19:59.603072Z","shell.execute_reply.started":"2023-07-24T13:19:59.595065Z","shell.execute_reply":"2023-07-24T13:19:59.599847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cfg = Box(cfg)\npprint(cfg)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:19:59.605003Z","iopub.execute_input":"2023-07-24T13:19:59.605433Z","iopub.status.idle":"2023-07-24T13:19:59.619184Z","shell.execute_reply.started":"2023-07-24T13:19:59.605391Z","shell.execute_reply":"2023-07-24T13:19:59.617224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed: int):  \n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \nseed_everything(cfg.SEED)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:19:59.621275Z","iopub.execute_input":"2023-07-24T13:19:59.621626Z","iopub.status.idle":"2023-07-24T13:19:59.633657Z","shell.execute_reply.started":"2023-07-24T13:19:59.621584Z","shell.execute_reply":"2023-07-24T13:19:59.632266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Delete unnecessary model files from WANDB artifacts\napi = wandb.Api(overrides={\n        \"project\": cfg.wandb.proj_name\n        })\n\nartifact_type, artifact_name = \"model\", ... \nfor version in api.artifact_versions(artifact_type, artifact_name):\n    print(version.aliases)\n    if len(version.aliases) == 0:\n        version.delete()","metadata":{"execution":{"iopub.status.busy":"2023-03-27T12:21:12.559981Z","iopub.execute_input":"2023-03-27T12:21:12.560367Z","iopub.status.idle":"2023-03-27T12:21:12.57023Z","shell.execute_reply.started":"2023-03-27T12:21:12.560337Z","shell.execute_reply":"2023-03-27T12:21:12.568791Z"},"_kg_hide-input":false}},{"cell_type":"markdown","source":"# DATASET","metadata":{}},{"cell_type":"code","source":"# https://pytorch.org/vision/main/_modules/torchvision/datasets/coco.html#CocoDetection\nclass CocoToSmpDataset(VisionDataset):\n    \"\"\"`MS Coco Detection <https://cocodataset.org/#detection-2016>`_ Dataset.\n\n    It requires the `COCO API to be installed <https://github.com/pdollar/coco/tree/master/PythonAPI>`_.\n\n    Args:\n        root (string): Root directory where images are downloaded to.\n        annFile (string): Path to json annotation file.\n        transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.PILToTensor``\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n        transforms (callable, optional): A function/transform that takes input sample and its target as entry\n            and returns a transformed version.\n    \"\"\"\n\n    def __init__(\n        self,\n        root: str,\n        annFile: str,\n        transform: Optional[Callable] = None,\n        target_transform: Optional[Callable] = None,\n        transforms: Optional[Callable] = None,\n    ) -> None:\n        super().__init__(root, transforms, transform, target_transform)\n        from pycocotools.coco import COCO\n\n        self.coco = COCO(annFile)\n        self.ids = list(sorted(self.coco.imgs.keys()))\n        \n    @staticmethod\n    def _transform_binarymask_to_distance_mask(gt_mask):\n        \"\"\"\n        Arguments:\n            gt_mask (ndarray): ground-truth binary mask in HW format\n        Returns:\n            distance_weight (ndarray): normalized between 0 and 1 and outer of object is 1\n            distance_weight_sum (float): sum af all normalized pixel values within inside of object in binary mask\n        \"\"\"\n        # PART 1: Object distance transform\n        dist = ndimage.distance_transform_cdt(gt_mask) # distance_transform_bf is less efficient\n        reverse_dist = dist.max() - dist\n        reverse_dist = (reverse_dist * gt_mask) # make outer background zero\n        \n        # CONSIDER EDGE CASES LIKE EMPTY AND FULL MASK\n        if reverse_dist.max() == 0:\n            if gt_mask.max() == 0: # Grount-truth mask is empty (hiç bir pixelde etiket yok)\n                # every element in dist is 0\n                # IoU/Distance loss is defined for non-empty classes\n                distance_weight = gt_mask # full of 0s. It doesn't matter since it will be zerout out in loss func\n                distance_weight_sum = np.sum(0)\n                return distance_weight, distance_weight_sum\n            elif gt_mask.min() == 1 : # Grount-truth mask is full (fotoğrafın her pixeli etiket)\n                # every element in dist is -1\n                distance_weight = gt_mask # full of 1s\n                distance_weight_sum = np.sum(distance_weight)\n                return distance_weight, distance_weight_sum\n            else:\n                # If 1s in mask is really less. Ie: small objects. Eg: gt_mask.sum() = 13\n                distance_weight = gt_mask # very less 1s, too many 0s\n                distance_weight_sum = np.sum(distance_weight)\n                return distance_weight, distance_weight_sum\n        \n        inner_reverse_dist_n = reverse_dist/ reverse_dist.max() # normalize it\n        # pprint(f\"Inner distance transform metrics: {inner_reverse_dist_n.min()}, {inner_reverse_dist_n.max()}, {inner_reverse_dist_n.mean()}\")\n\n        # PART 2: Outer distance transform\n        reverse_gt_mask = 1- gt_mask\n\n        # PART 3: Union of inner and outer transforms\n        distance_weight_vis = inner_reverse_dist_n + reverse_gt_mask # + outer_reverse_dist_n\n        # plt.imshow(distance_weight_vis)\n        distance_weight = inner_reverse_dist_n + reverse_gt_mask * -1 # + outer_reverse_dist_n * -1\n        distance_weight_sum = np.sum(inner_reverse_dist_n)\n\n        return distance_weight, distance_weight_sum\n\n    def _load_image(self, id: int) -> Image.Image:\n        path = self.coco.loadImgs(id)[0][\"file_name\"]\n        img = np.array(Image.open(os.path.join(self.root, path)).convert(\"RGB\"))\n        # If you need to convert to other format HWC -> CHW: img.transpose((-1, 0, 1))\n        return img\n    \n    def _load_mask(self, id: int) -> List[Any]:\n        if not self.coco.getAnnIds(id): # check for empty masks\n            # Empty mask case\n            h = self.coco.loadImgs(id)[0][\"height\"]\n            w = self.coco.loadImgs(id)[0][\"width\"]\n            arr_2dim = np.zeros((h, w)).astype(np.float32)\n            return arr_2dim # in HW format\n        else:\n             # Non-empty mask case\n            ann = self.coco.loadAnns(self.coco.getAnnIds(id))[0]\n            arr_2dim = self.coco.annToMask(ann=ann).astype(np.float32)\n            return arr_2dim # in HW format\n\n    def __getitem__(self, index: int) -> Tuple[Any, Any]:\n        id = self.ids[index]\n        image = self._load_image(id)\n        mask = self._load_mask(id)\n        \n        sample = dict(image=image, mask=mask)\n        if self.transforms is not None:\n            sample = self.transforms(**sample)    \n            sample[\"distance_mask\"], sample[\"distance_mask_sum\"] = self._transform_binarymask_to_distance_mask(np.array(sample[\"mask\"])) # change binary mask to distance transformed mask\n            sample[\"distance_mask\"] = np.expand_dims(sample[\"distance_mask\"], 0) # convert to CHW format ie. HW -> 1HW:\n            sample[\"mask\"] = np.expand_dims(sample[\"mask\"], 0) # convert to CHW format ie. HW -> 1HW:    \n        return sample\n    \n    def __len__(self) -> int:\n        return len(self.ids)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:44:48.386346Z","iopub.execute_input":"2023-07-24T13:44:48.386782Z","iopub.status.idle":"2023-07-24T13:44:48.415504Z","shell.execute_reply.started":"2023-07-24T13:44:48.386739Z","shell.execute_reply":"2023-07-24T13:44:48.414344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DatasetFromSubset(Dataset):\n    def __init__(self, subset, transforms=None):\n        self.subset = subset\n        self.transforms = transforms\n\n    @staticmethod\n    def _transform_binarymask_to_distance_mask(gt_mask):\n        \"\"\"\n        Arguments:\n            gt_mask (ndarray): ground-truth binary mask in HW format\n        Returns:\n            distance_weight (ndarray): normalized between 0 and 1 and outer of object is 1\n            distance_weight_sum (float): sum af all normalized pixel values within inside of object in binary mask\n        \"\"\"\n        # PART 1: Object distance transform\n        dist = ndimage.distance_transform_cdt(gt_mask) # distance_transform_bf is less efficient\n        reverse_dist = dist.max() - dist\n        reverse_dist = (reverse_dist * gt_mask) # make outer background zero\n        \n        # CONSIDER EDGE CASES LIKE EMPTY AND FULL MASK\n        if reverse_dist.max() == 0:\n            if gt_mask.max() == 0: # Grount-truth mask is empty (hiç bir pixelde etiket yok)\n                # every element in dist is 0\n                # IoU/Distance loss is defined for non-empty classes\n                distance_weight = gt_mask # full of 0s. It doesn't matter since it will be zerout out in loss func\n                distance_weight_sum = np.sum(0)\n                return distance_weight, distance_weight_sum\n            elif gt_mask.min() == 1 : # Grount-truth mask is full (fotoğrafın her pixeli etiket)\n                # every element in dist is -1\n                distance_weight = gt_mask # full of 1s\n                distance_weight_sum = np.sum(distance_weight)\n                return distance_weight, distance_weight_sum\n            else:\n                # If 1s in mask is really less. Ie: small objects. Eg: gt_mask.sum() = 13\n                distance_weight = gt_mask # very less 1s, too many 0s\n                distance_weight_sum = np.sum(distance_weight)\n                return distance_weight, distance_weight_sum\n        \n        inner_reverse_dist_n = reverse_dist/ reverse_dist.max() # normalize it\n        # pprint(f\"Inner distance transform metrics: {inner_reverse_dist_n.min()}, {inner_reverse_dist_n.max()}, {inner_reverse_dist_n.mean()}\")\n\n        # PART 2: Outer distance transform\n        reverse_gt_mask = 1- gt_mask\n\n        # PART 3: Union of inner and outer transforms\n        distance_weight_vis = inner_reverse_dist_n + reverse_gt_mask # + outer_reverse_dist_n\n        # plt.imshow(distance_weight_vis)\n        distance_weight = inner_reverse_dist_n + reverse_gt_mask * -1 # + outer_reverse_dist_n * -1\n        distance_weight_sum = np.sum(inner_reverse_dist_n)\n\n        return distance_weight, distance_weight_sum\n    \n    def __getitem__(self, index):\n        sample = self.subset[index]\n        if self.transforms:\n            sample = self.transforms(**sample)  \n\n        sample[\"distance_mask\"], sample[\"distance_mask_sum\"] = self._transform_binarymask_to_distance_mask(np.array(sample[\"mask\"])) # change binary mask to distance transformed mask\n        sample[\"distance_mask\"] = np.expand_dims(sample[\"distance_mask\"], 0) # convert to CHW format ie. HW -> 1HW:        \n        sample[\"mask\"] = np.expand_dims(sample[\"mask\"], 0) # convert to CHW format ie. HW -> 1HW:\n        return sample\n\n    def __len__(self):\n        return len(self.subset)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:44:48.809581Z","iopub.execute_input":"2023-07-24T13:44:48.809946Z","iopub.status.idle":"2023-07-24T13:44:48.826408Z","shell.execute_reply.started":"2023-07-24T13:44:48.809913Z","shell.execute_reply":"2023-07-24T13:44:48.825217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_transform = A.Compose([\n    # A.Resize(height=cfg.transform.image_resize_h, width=cfg.transform.image_resize_w),\n    A.LongestMaxSize(max(cfg.transform.image_resize_h, cfg.transform.image_resize_w)),\n    A.PadIfNeeded(min_height=cfg.transform.image_resize_h, min_width=cfg.transform.image_resize_w, border_mode=cv2.BORDER_REFLECT_101),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), # use ImageNet image normalization \n    ToTensorV2() # numpy HWC image is converted to pytorch CHW tensor\n])\n\nval_transform = A.Compose([\n    # A.Resize(height=cfg.transform.image_resize_h, width=cfg.transform.image_resize_w),\n    A.LongestMaxSize(max(cfg.transform.image_resize_h, cfg.transform.image_resize_w)),\n    A.PadIfNeeded(min_height=cfg.transform.image_resize_h, min_width=cfg.transform.image_resize_w, border_mode=cv2.BORDER_CONSTANT),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), # use ImageNet image normalization \n    ToTensorV2() # numpy HWC image is converted to pytorch CHW tensor\n])\n\ntest_transform = A.Compose([\n    A.LongestMaxSize(max(cfg.transform.image_resize_h, cfg.transform.image_resize_w)),\n    A.PadIfNeeded(min_height=cfg.transform.image_resize_h, min_width=cfg.transform.image_resize_w, border_mode=cv2.BORDER_CONSTANT),\n    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)), # use ImageNet image normalization \n    ToTensorV2() # numpy HWC image is converted to pytorch CHW tensor\n])\n\nunnorm_transform = A.Compose([\n    A.Normalize(mean=(-0.485/0.229, -0.456/0.224, -0.406/0.225), std=(1.0/0.229, 1.0/0.224, 1.0/0.225), max_pixel_value=1.0),\n    ToTensorV2()\n    ]) # -mean / std, 1.0 / std for unnormalization","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:44:49.019270Z","iopub.execute_input":"2023-07-24T13:44:49.019677Z","iopub.status.idle":"2023-07-24T13:44:49.034394Z","shell.execute_reply.started":"2023-07-24T13:44:49.019643Z","shell.execute_reply":"2023-07-24T13:44:49.033267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DISTANCE LOSS","metadata":{}},{"cell_type":"code","source":"# https://github.com/qubvel/segmentation_models.pytorch/blob/master/segmentation_models_pytorch/losses/constants.py\n\n#: Loss binary mode suppose you are solving binary segmentation task.\n#: That mean yor have only one class which pixels are labled as **1**,\n#: the rest pixels are background and labeled as **0**.\n#: Target mask shape - (N, H, W), model output mask shape (N, 1, H, W).\nBINARY_MODE: str = \"binary\"\n\n#: Loss multiclass mode suppose you are solving multi-**class** segmentation task.\n#: That mean you have *C = 1..N* classes which have unique label values,\n#: classes are mutually exclusive and all pixels are labeled with theese values.\n#: Target mask shape - (N, H, W), model output mask shape (N, C, H, W).\nMULTICLASS_MODE: str = \"multiclass\"\n\n#: Loss multilabel mode suppose you are solving multi-**label** segmentation task.\n#: That mean you have *C = 1..N* classes which pixels are labeled as **1**,\n#: classes are not mutually exclusive and each class have its own *channel*,\n#: pixels in each channel which are not belong to class labeled as **0**.\n#: Target mask shape - (N, C, H, W), model output mask shape (N, C, H, W).\nMULTILABEL_MODE: str = \"multilabel\"","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:03:36.208049Z","iopub.execute_input":"2023-07-24T13:03:36.208487Z","iopub.status.idle":"2023-07-24T13:03:36.216151Z","shell.execute_reply.started":"2023-07-24T13:03:36.208452Z","shell.execute_reply":"2023-07-24T13:03:36.215002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://github.com/qubvel/segmentation_models.pytorch/blob/master/segmentation_models_pytorch/losses/_functional.py\n\ndef to_tensor(x, dtype=None) -> torch.Tensor:\n    if isinstance(x, torch.Tensor):\n        if dtype is not None:\n            x = x.type(dtype)\n        return x\n    if isinstance(x, np.ndarray):\n        x = torch.from_numpy(x)\n        if dtype is not None:\n            x = x.type(dtype)\n        return x\n    if isinstance(x, (list, tuple)):\n        x = np.array(x)\n        x = torch.from_numpy(x)\n        if dtype is not None:\n            x = x.type(dtype)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:03:36.425106Z","iopub.execute_input":"2023-07-24T13:03:36.425482Z","iopub.status.idle":"2023-07-24T13:03:36.436884Z","shell.execute_reply.started":"2023-07-24T13:03:36.425451Z","shell.execute_reply":"2023-07-24T13:03:36.435704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def soft_distance_score(\n    output: torch.Tensor,\n    target: torch.Tensor,\n    target_sum: float,\n    smooth: float = 0.0,\n    eps: float = 1e-7,\n    dims=None,\n) -> torch.Tensor:\n    assert output.size() == target.size()\n    if dims is not None:\n        intersection = torch.sum(output * target, dim=dims)\n        cardinality = torch.sum(target_sum) # torch.sum(target_sum, dim=dims)\n    else:\n        intersection = torch.sum(output * target)\n        cardinality = torch.sum(target_sum)\n        \n    distance_score = intersection / (cardinality + smooth).clamp_min(eps)\n    return distance_score","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:03:36.575542Z","iopub.execute_input":"2023-07-24T13:03:36.575884Z","iopub.status.idle":"2023-07-24T13:03:36.584092Z","shell.execute_reply.started":"2023-07-24T13:03:36.575836Z","shell.execute_reply":"2023-07-24T13:03:36.582975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from typing import Optional, List\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.modules.loss import _Loss\n\n__all__ = [\"DistanceLoss\"]\n\nclass DistanceLoss(_Loss):\n    def __init__(\n        self,\n        mode: str,\n        classes: Optional[List[int]] = None,\n        log_loss: bool = False,\n        from_logits: bool = True,\n        smooth: float = 0.0,\n        eps: float = 1e-7,\n    ):\n        \"\"\"Distance loss for image segmentation task.\n        It supports binary, multiclass and multilabel cases\n\n        Args:\n            mode: Loss mode 'binary', 'multiclass' or 'multilabel'\n            classes:  List of classes that contribute in loss computation. By default, all channels are included.\n            log_loss: If True, loss computed as `- log(jaccard_coeff)`, otherwise `1 - jaccard_coeff`\n            from_logits: If True, assumes input is raw logits\n            smooth: Smoothness constant for dice coefficient\n            eps: A small epsilon for numerical stability to avoid zero division error\n                (denominator will be always greater or equal to eps)\n\n        Shape\n             - **y_pred** - torch.Tensor of shape (N, C, H, W)\n             - **y_true** - torch.Tensor of shape (N, H, W) or (N, C, H, W)\n\n        Reference\n            # https://github.com/BloodAxe/pytorch-toolbelt\n        \"\"\"\n        assert mode in {BINARY_MODE, MULTILABEL_MODE, MULTICLASS_MODE}\n        super(DistanceLoss, self).__init__()\n\n        self.mode = mode\n        if classes is not None:\n            assert mode != BINARY_MODE, \"Masking classes is not supported with mode=binary\"\n            classes = to_tensor(classes, dtype=torch.long)\n\n        self.classes = classes\n        self.from_logits = from_logits\n        self.smooth = smooth\n        self.eps = eps\n        self.log_loss = log_loss\n\n    def forward(self, y_pred: torch.Tensor, y_true: torch.Tensor, y_true_sum: torch.Tensor) -> torch.Tensor:\n\n        assert y_true.size(0) == y_pred.size(0)\n\n        if self.from_logits:\n            # Apply activations to get [0..1] class probabilities\n            # Using Log-Exp as this gives more numerically stable result and does not cause vanishing gradient on\n            # extreme values 0 and 1\n            if self.mode == MULTICLASS_MODE:\n                y_pred = y_pred.log_softmax(dim=1).exp()\n            else:\n                y_pred = F.logsigmoid(y_pred).exp()\n\n        bs = y_true.size(0)\n        num_classes = y_pred.size(1)\n        dims = (0, 2)\n\n        if self.mode == BINARY_MODE:\n            y_true = y_true.view(bs, 1, -1)\n            y_pred = y_pred.view(bs, 1, -1)\n\n        if self.mode == MULTICLASS_MODE:\n            y_true = y_true.view(bs, -1)\n            y_pred = y_pred.view(bs, num_classes, -1)\n\n            y_true = F.one_hot(y_true, num_classes)  # N,H*W -> N,H*W, C\n            y_true = y_true.permute(0, 2, 1)  # N, C, H*W\n\n        if self.mode == MULTILABEL_MODE:\n            y_true = y_true.view(bs, num_classes, -1)\n            y_pred = y_pred.view(bs, num_classes, -1)\n        \n        scores = soft_distance_score(\n            y_pred,\n            y_true.type_as(y_pred), # y_true.type(y_pred.dtype)\n            target_sum=y_true_sum,\n            smooth=self.smooth,\n            eps=self.eps,\n            dims=dims,\n        )\n        if self.log_loss:\n            loss = -torch.log(scores.clamp_min(self.eps))\n        else:\n            loss = 1.0 - scores\n\n        # IoU loss is defined for non-empty classes\n        # So we zero contribution of channel that does not have true pixels\n        # NOTE: A better workaround would be to use loss term `mean(y_pred)`\n        # for this case, however it will be a modified jaccard loss\n        \n        mask = y_true_sum.sum() > 0 # y_true.sum(dims) > 0\n        loss *= mask.to(loss.dtype) # mask.float()\n            \n        if self.classes is not None:\n            loss = loss[self.classes]\n\n        return loss.mean()","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:03:36.779340Z","iopub.execute_input":"2023-07-24T13:03:36.779961Z","iopub.status.idle":"2023-07-24T13:03:36.803636Z","shell.execute_reply.started":"2023-07-24T13:03:36.779926Z","shell.execute_reply":"2023-07-24T13:03:36.802557Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SCHEDULER","metadata":{}},{"cell_type":"code","source":"class ReduceLROnPlateauOptimized(ReduceLROnPlateau):\n    \"\"\"\n    Bunu implemente edince ReduceLROnPlateau ile aynı şekilde çalışıyor\n    \"\"\"\n    def __init__(self, optimizer, **kwargs):\n        super().__init__(optimizer, **kwargs)\n        \n    def _reduce_lr(self, epoch):\n        for i, param_group in enumerate(self.optimizer.param_groups):\n            old_lr = float(param_group['lr'])\n            new_lr = max(old_lr * self.factor, self.min_lrs[i])\n            if old_lr - new_lr > self.eps:\n                param_group['lr'] = new_lr\n                if self.verbose:\n                    epoch_str = (\"%.2f\" if isinstance(epoch, float) else\n                                 \"%.5d\") % epoch\n                    print('Epoch {}: reducing learning rate'\n                          ' of group {} to {:.4e}.'.format(epoch_str, i, new_lr)) \n                return new_lr\n            \n    def step(self, metrics, epoch=None):\n        # convert `metrics` to float, in case it's a zero-dim Tensor\n        current = float(metrics)\n        if epoch is None:\n            epoch = self.last_epoch + 1\n        else:\n            warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n        self.last_epoch = epoch\n\n        if self.is_better(current, self.best):\n            self.best = current\n            self.num_bad_epochs = 0\n        else:\n            self.num_bad_epochs += 1\n\n        if self.in_cooldown:\n            self.cooldown_counter -= 1\n            self.num_bad_epochs = 0  # ignore any bad epochs in cooldown\n\n        if self.num_bad_epochs > self.patience:\n            new_lr = self._reduce_lr(epoch)\n            self.cooldown_counter = self.cooldown\n            self.num_bad_epochs = 0\n\n        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]\n        \n        if 'new_lr' in locals():\n            return new_lr","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:03:37.451318Z","iopub.execute_input":"2023-07-24T13:03:37.452076Z","iopub.status.idle":"2023-07-24T13:03:37.467652Z","shell.execute_reply.started":"2023-07-24T13:03:37.452035Z","shell.execute_reply":"2023-07-24T13:03:37.466508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BOUNDARY IOU TENSOR IMPLEMENTATION","metadata":{}},{"cell_type":"code","source":"# General util function to get the boundary of a binary mask.\ndef mask_to_boundary_tensor(mask, dilation_ratio=0.02):\n    \"\"\"\n    Convert binary mask to boundary mask.\n    Arguments:\n        mask (torch.Tensor): mask of torch.Tensor of shape (N, C, H, W)\n        dilation_ratio (float): ratio to calculate dilation = dilation_ratio * image_diagonal\n    Returns: \n        boundary mask (torch.Tensor): boundary mask of torch.Tensor of shape (N, C, H, W)\n    \"\"\"\n    nb, c, h, w = mask.shape\n    img_diag = np.sqrt(h ** 2 + w ** 2)\n    dilation = int(round(dilation_ratio * img_diag))\n    if dilation < 1:\n        dilation = 1\n        \n    boundary_mask = torch.zeros_like(mask)\n    kernel = np.ones((3, 3), dtype=np.uint8)\n    for idx, img_tensor in enumerate(mask[:,]): # tensor_img will be in the shape of (C, H, W)\n        # From tensor to numpy\n        # .cpu().detach().squeeze().numpy()\n        img_np = np.array(img_tensor.cpu().detach(), dtype=\"uint8\").transpose(1, 2, 0).squeeze() # in the shape of (H, W, C) -> (H, W) since C=1\n        # Pad image so mask truncated by the image border is also considered as boundary.\n        new_mask = cv2.copyMakeBorder(img_np, 1, 1, 1, 1, cv2.BORDER_CONSTANT, value=0)\n        new_mask_erode = cv2.erode(new_mask, kernel, iterations=dilation)\n        mask_erode = new_mask_erode[1 : h + 1, 1 : w + 1] # in the shape of (H, W)\n        boundary_mask_np = img_np - mask_erode\n        boundary_mask_np = np.expand_dims(boundary_mask_np, 0)\n        # import pdb; pdb.set_trace()\n        # From numpy to tensor\n        boundary_mask[idx,] = torch.tensor(boundary_mask_np)\n    return boundary_mask","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:03:37.796948Z","iopub.execute_input":"2023-07-24T13:03:37.797364Z","iopub.status.idle":"2023-07-24T13:03:37.810053Z","shell.execute_reply.started":"2023-07-24T13:03:37.797330Z","shell.execute_reply":"2023-07-24T13:03:37.808831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MODEL","metadata":{}},{"cell_type":"markdown","source":"This is just regular PyTorch organized in a specific format.\n\n Notice the following:\n\n* no GPU/TPU specific code\n* no .to(device)\n* np .cuda()","metadata":{}},{"cell_type":"code","source":"# https://pytorch-lightning.readthedocs.io/en/stable/guides/speed.html\nclass ImageSegModel(pl.LightningModule):\n    def __init__(self, cfg, train_transform, val_transform, test_transform, unnorm_transform):\n        '''method used to define our model and data parameters'''\n        super().__init__()    \n        # Set our init args as class attributes\n        self.n_cpu = os.cpu_count()\n        self.cfg = cfg\n        \n        # Define PyTorch model, model weights are in cuda\n        encoder_weights = \"imagenet\"\n        input_channels = 3\n        self.model = smp.create_model(arch=cfg.model.model_name,\n                                      encoder_name=cfg.model.encoder_name,\n                                      encoder_weights=encoder_weights,\n                                      in_channels=input_channels,\n                                      classes=cfg.output_class_num)\n        \n        # Preprocessing/Transformations  \n        self.train_transform = train_transform\n        self.val_transform = val_transform\n        self.test_transform = test_transform\n        self.unnorm_transform = unnorm_transform\n        \n        # Define loss. If predicted mask contains logits, and loss param `from_logits` is set to True\n        if cfg.distance_transform_loss:\n            self.loss_distance = DistanceLoss(BINARY_MODE, from_logits=True)\n        else:\n            self.loss_dice = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n            # self.loss_bce = BCEWithLogitsLoss()\n            # self.loss_dice = smp.losses.DiceLoss(smp.losses.BINARY_MODE, from_logits=True)\n            # self.loss_focal = smp.losses.FocalLoss(smp.losses.BINARY_MODE) # it uses focal_loss_with_logits\n            # self.loss_iou = smp.losses.JaccardLoss(smp.losses.BINARY_MODE, from_logits=False)\n        self.distance_transform_metric = DistanceLoss(BINARY_MODE, from_logits=False)\n        \n        self.save_hyperparameters(ignore=[\"model\"])\n    \n    def forward(self, x):\n        '''method used for inference input -> output'''\n        return self.model(x) # logits\n\n    def _shared_step(self, batch, stage):\n        '''needs to return a loss from a single batch'''\n        x, y, y_distance, y_distance_sum = batch[\"image\"], batch[\"mask\"], batch[\"distance_mask\"], batch[\"distance_mask_sum\"]\n            \n        # Shape of the image should be [batch_size, num_channels, height, width]\n        # if you work with grayscale images, expand channels dim to have [batch_size, 1, height, width]\n        assert x.ndim == 4\n    \n        # Shape of the mask should be [batch_size, num_classes, height, width]. For binary segmentation num_classes=1\n        assert y.ndim == 4\n        assert y_distance.ndim == 4\n        \n        # Check that image dimensions are divisible by 32, encoder and decoder connected by `skip connections`.\n        # Usually encoder have 5 stages of downsampling by factor 2 (2 ^ 5 = 32); e.g. if we have image with shape 65x65 we will have \n        # following shapes of features in encoder and decoder: 84, 42, 21, 10, 5 -> 5, 10, 20, 40, 80 and we will get an error trying to concat these features\n        h, w = x.shape[2:]\n        assert h % 32 == 0 and w % 32 == 0\n        \n        # Check that mask values in between 0 and 1, NOT 0 and 255 for binary segmentation\n        assert y.max() <= 1.0 and y.min() >= 0\n        assert y_distance.max() <= 1.0 and y_distance.min() >= -1    \n        \n        logits_y = self(x) # it is actually self.forward(x)\n        # Lets compute metrics for some threshold. First convert mask values to probabilities, then apply thresholding\n        prob_y = logits_y.sigmoid()\n        pred_y_th = (prob_y > 0.95).float()\n        \n        boundary_y = mask_to_boundary_tensor(y, dilation_ratio=0.02)\n        boundary_pred_y = mask_to_boundary_tensor(prob_y, dilation_ratio=0.02)\n        \n        if cfg.distance_transform_loss:\n            loss = self.loss_distance(logits_y, y_distance, y_distance_sum)\n        else:\n            loss_dice = self.loss_dice(logits_y, y)\n            # loss_bce = self.loss_bce(logits_y, y)\n            loss = loss_dice\n            # loss = loss_dice + loss_bce\n            # loss_dice = self.loss_dice(logits_y, y)\n            # loss_focal = self.loss_focal(logits_y, y)\n            # loss = loss_dice + loss_focal\n            # if self.current_epoch > -1:\n            #     loss_iou = self.loss_iou(prob_y, y)\n            #     loss_biou = self.loss_iou(boundary_pred_y, boundary_y)\n            #     loss = torch.maximum(loss_iou, loss_biou)\n            # else:\n            #     loss = self.loss_iou(prob_y, y)\n        self.log(f\"{stage}_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n\n        # We will compute IoU metric by two ways\n        #   1. dataset-wise\n        #   2. image-wise\n        # but for now we just compute true positive, false positive, false negative and true negative 'pixels' for each image and class\n        # these values will be aggregated in the end of an epoch\n        tp, fp, fn, tn = smp.metrics.get_stats(pred_y_th.long(), y.long(), mode=self.cfg.mode, num_classes=self.cfg.output_class_num)\n        \n        # Extract tp, fp, fn, tn for boundary iou \n        boundary_pred_y_th = mask_to_boundary_tensor(pred_y_th, dilation_ratio=0.02)\n        b_tp, b_fp, b_fn, b_tn = smp.metrics.get_stats(boundary_pred_y_th.long(), boundary_y.long(), mode=\"binary\")\n        \n        # Calculate distance transform evaluation metric here. Do not concaterate in the epoch_end and then evaluate since it will require more computation.\n        distance_transform_metric = 1- self.distance_transform_metric(pred_y_th, y_distance, y_distance_sum)\n        self.log(f\"{stage}_distance_transform_evalmetric\", distance_transform_metric, on_step=False, on_epoch=True, prog_bar=True)\n        \n        return {\n            \"loss\": loss,\n            \"tp\": tp,\n            \"fp\": fp,\n            \"fn\": fn,\n            \"tn\": tn,\n            \"b_tp\": b_tp,\n            \"b_fp\": b_fp,\n            \"b_fn\": b_fn,\n            \"b_tn\": b_tn,\n            \"prob_y\": prob_y\n        }\n     \n    def training_step(self, batch, batch_idx):\n        return self._shared_step(batch, \"train\")\n \n    def validation_step(self, batch, batch_idx):\n        return self._shared_step(batch, \"val\")\n    \n    def test_step(self, batch, batch_idx):\n        return self._shared_step(batch, \"test\")\n\n    def _shared_step_end(self, batch_parts):\n        # Check if there is multi-gpu or not. This step must be here due to pl lib.\n        if batch_parts[\"loss\"].numel() != 1: # multi-gpu case\n            batch_parts[\"loss\"] = torch.mean(batch_parts[\"loss\"], dim=0) # take the mean of losses from different GPUs\n        return batch_parts\n    \n    def training_step_end(self, batch_parts):\n        return self._shared_step_end(batch_parts) \n\n    def validation_step_end(self, batch_parts):\n        return self._shared_step_end(batch_parts) \n    \n    def test_step_end(self, batch_parts):\n        return self._shared_step_end(batch_parts) \n\n    def _shared_epoch_end(self, step_outputs, stage):\n        # outputs are coming from the result or trainin/validation/test steps respectively\n        # aggregate step metrics\n        tp = torch.cat([x[\"tp\"] for x in step_outputs])\n        fp = torch.cat([x[\"fp\"] for x in step_outputs])\n        fn = torch.cat([x[\"fn\"] for x in step_outputs])\n        tn = torch.cat([x[\"tn\"] for x in step_outputs])\n        b_tp = torch.cat([x[\"b_tp\"] for x in step_outputs])\n        b_fp = torch.cat([x[\"b_fp\"] for x in step_outputs])\n        b_fn = torch.cat([x[\"b_fn\"] for x in step_outputs])\n        b_tn = torch.cat([x[\"b_tn\"] for x in step_outputs])\n\n        # per image IoU means that we first calculate IoU score for each image and then compute mean over these scores\n        per_image_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n        \n        # dataset IoU means that we aggregate intersection and union over whole dataset and then compute IoU score. \n        # The difference between dataset_iou and per_image_iou scores in this particular case will not be much, \n        # however for dataset with \"empty\" images (images without target class) a large gap could be observed. \n        # Empty images influence a lot on per_image_iou and much less on dataset_iou.\n        dataset_iou = smp.metrics.iou_score(tp, fp, fn, tn, reduction=\"micro\")\n        \n        per_image_dice = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n        dataset_dice = smp.metrics.f1_score(tp, fp, fn, tn, reduction=\"micro\")\n        \n        per_image_fbeta = smp.metrics.fbeta_score(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n        dataset_fbeta = smp.metrics.fbeta_score(tp, fp, fn, tn, reduction=\"micro\")\n        \n        per_image_sensitivity= smp.metrics.sensitivity(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n        dataset_sensitivity = smp.metrics.sensitivity(tp, fp, fn, tn, reduction=\"micro\")\n        \n        per_image_specificity = smp.metrics.specificity(tp, fp, fn, tn, reduction=\"micro-imagewise\")\n        dataset_specificity = smp.metrics.specificity(tp, fp, fn, tn, reduction=\"micro\")\n        \n        per_image_bIoU = smp.metrics.iou_score(b_tp, b_fp, b_fn, b_tn, reduction=\"micro-imagewise\")\n        dataset_bIoU = smp.metrics.iou_score(b_tp, b_fp, b_fn, b_tn, reduction=\"micro\")\n        \n        metrics = {\n            f\"{stage}_per_image_iou\": per_image_iou,\n            f\"{stage}_dataset_iou\": dataset_iou,\n            f\"{stage}_per_image_dice\": per_image_dice,\n            f\"{stage}_dataset_dice\": dataset_dice,\n            f\"{stage}_per_image_fbeta\": per_image_fbeta,\n            f\"{stage}_dataset_fbeta\": dataset_fbeta,\n            f\"{stage}_per_image_sensitivity\": per_image_sensitivity,\n            f\"{stage}_dataset_sensitivity\": dataset_sensitivity,\n            f\"{stage}_per_image_specificity\": per_image_specificity,\n            f\"{stage}_dataset_specificity\": dataset_specificity,\n            f\"{stage}_per_image_bIoU\": per_image_bIoU,\n            f\"{stage}_dataset_bIoU\": dataset_bIoU,\n        }\n        \n        self.log_dict(metrics, prog_bar=True)\n    \n    def training_epoch_end(self, training_step_outputs):\n        return self._shared_epoch_end(training_step_outputs, \"train\")\n\n    def validation_epoch_end(self, validation_step_outputs):\n        return self._shared_epoch_end(validation_step_outputs, \"val\")\n\n    def test_epoch_end(self, test_step_outputs):\n        return self._shared_epoch_end(test_step_outputs, \"test\")  \n            \n    def configure_optimizers(self):\n        '''defines model optimizer and scheduler'''\n        optimizer = torch.optim.AdamW(self.parameters(), lr=self.cfg.optimizer.lr)\n        if self.cfg.optimizer.reduce_rl_on:          \n            # It is called automatic optimization if `scheduler.step()` is not called manually inside pl.LightningModule. Otherwise, it is manuel optimization\n            scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=5, factor=0.5, min_lr=1e-7) # ReduceLROnPlateauOptimized\n            return {\n                \"optimizer\": optimizer,\n                \"lr_scheduler\": {\n                    \"scheduler\": scheduler,\n                    \"interval\": \"epoch\", # The unit of the scheduler's step size, could also be 'step'. 'epoch' updates the scheduler on epoch end whereas 'step' updates it after a optimizer update.\n                    \"monitor\": \"val_loss\", # Metric to monitor for schedulers like ReduceLROnPlateau\n                    \"frequency\": 1 # How many epochs/steps should pass between calls to `scheduler.step()`. 1 corresponds to updating the learning rate after every epoch/step. \n                    # If \"monitor\" references validation metrics, then \"frequency\" should be set to a multiple of \"trainer.check_val_every_n_epoch\".\n                    # \"frequency\" and \"interval\" will be ignored even if they are provided in here configure_optimizers() during manual optimization\n                },\n            }\n        else:\n            return optimizer\n    ####################\n    # DATA RELATED HOOKS\n    ####################\n\n    def setup(self, stage=None):\n        # Assign train/val datasets for use in dataloaders\n        if stage == \"fit\" or stage is None:\n            full_ds = CocoToSmpDataset(root=os.path.join(self.cfg.data_dir, \"train\"), \n                                       annFile=os.path.join(self.cfg.data_dir, \"annotations_train.json\")\n                                       )\n            # Train-val split before appliying transformations\n            train_subset, val_subset = random_split(full_ds, [0.8, 0.2],\n                                                      generator=torch.Generator().manual_seed(self.cfg.SEED))\n            # Apply transformations to each subset\n            self.train_ds = DatasetFromSubset(train_subset, transforms=self.train_transform)\n            self.val_ds = DatasetFromSubset(val_subset, transforms=self.val_transform)\n        # Assign test dataset for use in dataloader(s)\n        if stage == \"test\" or stage is None:\n            self.test_ds = CocoToSmpDataset(root=os.path.join(self.cfg.data_dir, \"test\"), \n                                            annFile=os.path.join(self.cfg.data_dir, \"annotations_test.json\"),\n                                            transforms=self.test_transform)\n            \n    def train_dataloader(self):\n        return DataLoader(self.train_ds, batch_size=self.cfg.train_dl.batch_size, shuffle=True, num_workers=self.n_cpu, pin_memory=True)\n        # pin_memory will put the fetched data Tensors in pinned memory, and thus enables faster data transfer to CUDA-enabled GPUs\n\n    def val_dataloader(self):\n        return DataLoader(self.val_ds, batch_size=self.cfg.val_dl.batch_size, shuffle=False, num_workers=self.n_cpu, pin_memory=True)\n\n    def test_dataloader(self):\n        return DataLoader(self.test_ds, batch_size=self.cfg.test_dl.batch_size, shuffle=False, num_workers=self.n_cpu, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:03:38.374651Z","iopub.execute_input":"2023-07-24T13:03:38.375037Z","iopub.status.idle":"2023-07-24T13:03:38.437618Z","shell.execute_reply.started":"2023-07-24T13:03:38.375002Z","shell.execute_reply":"2023-07-24T13:03:38.434696Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://pytorch-lightning.readthedocs.io/en/stable/common/lightning_module.html#on-validation-epoch-end\nclass ReduceLROnPlateauOptCallback(Callback): \n    def on_validation_epoch_end(self, trainer, pl_module):\n        \"\"\"Called when the validation epoch ends.\"\"\"\n        # Manual optimization. Note that step should be called after val_loss is calculated\n        sch = pl_module.lr_schedulers()\n        # If the selected scheduler is a ReduceLROnPlateau scheduler.\n        if isinstance(sch, ReduceLROnPlateau):\n            new_lr = sch.step(pl_module.trainer.callback_metrics[\"val_loss\"])\n            if new_lr is None:\n                print(\"Regular ReduceLROnPlateau\")\n            elif isinstance(new_lr, float):\n                print(f\"Optimized ReduceLROnPlateau with new lr of {new_lr}\")\n                trainer.fit(pl_module, ckpt_path=\"best\")","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:03:38.443430Z","iopub.execute_input":"2023-07-24T13:03:38.443747Z","iopub.status.idle":"2023-07-24T13:03:38.451128Z","shell.execute_reply.started":"2023-07-24T13:03:38.443719Z","shell.execute_reply":"2023-07-24T13:03:38.449736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# https://docs.wandb.ai/guides/integrations/lightning\n# to control when you log to Weights & Biases via the WandbLogger\nclass LogSegPredictionCallback(Callback): \n    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n        \"\"\"Called when the validation batch ends.\"\"\"\n        # outputs: The outputs of validation_step_end(validation_step(x))\n        # Let's log all sample image predictions from the fifth batch\n        if batch_idx == pl_module.cfg.vis_val_batch_id: \n            with torch.no_grad():   \n                # x, y = batch[\"image\"], batch[\"mask\"]\n                x, y, y_distance, y_distance_sum = batch[\"image\"], batch[\"mask\"], batch[\"distance_mask\"], batch[\"distance_mask_sum\"]\n                \n                # All five images and preds in one figure\n                fig1, axs1 = plt.subplots(pl_module.cfg.vis_img_num, 5, figsize=(50, 50)) # pl_module.cfg.val_dl.batch_size\n                for img_id, (img_gt, mask_gt, mask_dist_gt, mask_pred) in enumerate(zip(x, y, y_distance, outputs[\"prob_y\"])): # [1, num_channels, height, width]\n                    mask_pred_th = (mask_pred > 0.5).float()\n                    un_img = img_gt.cpu().numpy().squeeze().transpose(1, 2, 0) # for visualization we have to transpose back to HWC\n                    img_gt = (pl_module.unnorm_transform(image=un_img)[\"image\"]*255).to(torch.uint8)\n                    \n                    axs1[img_id, 0].imshow(np.asarray(visF.to_pil_image(img_gt)))\n                    axs1[img_id, 0].set_title(\"Image\", fontsize=30)\n\n                    axs1[img_id, 1].imshow(mask_dist_gt.cpu().numpy().squeeze())  # for visualization we have to remove 3rd dimension of mask\n                    axs1[img_id, 1].set_title(\"GT-DistanceMask\", fontsize=30)\n                    \n                    axs1[img_id, 2].imshow(mask_gt.cpu().numpy().squeeze())  # for visualization we have to remove 3rd dimension of mask\n                    axs1[img_id, 2].set_title(\"GT-Mask\", fontsize=30)\n    \n                    axs1[img_id, 3].imshow(mask_pred.cpu().numpy().squeeze())  # for visualization we have to remove 3rd dimension of mask\n                    axs1[img_id, 3].set_title(\"Pred-Mask\", fontsize=30)\n                        \n                    axs1[img_id, 4].imshow(mask_pred_th.cpu().numpy().squeeze())  # for visualization we have to remove 3rd dimension of mask\n                    axs1[img_id, 4].set_title(\"Pred-Mask Thresholded\", fontsize=30)\n                    \n                    # Save only first 4 images in the batch\n                    if img_id == pl_module.cfg.vis_img_num - 1:\n                        break\n                plt.show()\n                img_dir = os.path.join(\"/kaggle/working/\", f\"epoch{pl_module.current_epoch}_data_pred_vis.png\")\n                fig1.savefig(img_dir, bbox_inches='tight') \n                wandb_logger.log_image(key=f\"val_batch{batch_idx}_all\", images=[img_dir])\n                \n                # Four images and their preds are in different figures\n                fig2, axs2 = plt.subplots(2, 2, figsize=(30, 30)) \n                for img_id, (img_gt, mask_gt, mask_pred) in enumerate(zip(x, y, outputs[\"prob_y\"])): # [1, num_channels, height, width]\n                    mask_pred_th = (mask_pred > 0.5).squeeze(0)\n                    mask_gt_th = mask_gt > 0.5 # boolean\n                    \n                    un_img = img_gt.cpu().numpy().squeeze().transpose(1, 2, 0) # for visualization we have to transpose back to HWC\n                    img = (pl_module.unnorm_transform(image=un_img)[\"image\"]*255).to(torch.uint8)\n                         \n                    gt_mask_on_img = draw_segmentation_masks(image=img, masks=mask_gt_th, alpha=0.7, colors=[\"orange\"]) # Tensor[C, H, W]\n                    pred_mask_on_img = draw_segmentation_masks(image=img, masks=mask_pred_th, alpha=0.7, colors=[\"orange\"]) # Tensor[C, H, W]\n                    \n                    axs2[0, 0].imshow(np.asarray(visF.to_pil_image(gt_mask_on_img)))\n                    axs2[0, 0].set_title(\"GT-Mask on Image\", fontsize=30)\n                    \n                    axs2[0, 1].imshow(np.asarray(visF.to_pil_image(pred_mask_on_img)))\n                    axs2[0, 1].set_title(\"Pred-Mask on Image\", fontsize=30)   \n        \n                    axs2[1, 0].imshow(mask_gt.cpu().numpy().squeeze())  # for visualization we have to remove 3rd dimension of mask\n                    axs2[1, 0].set_title(\"GT-Mask\", fontsize=30)\n                        \n                    axs2[1, 1].imshow(mask_pred.cpu().numpy().squeeze())  # for visualization we have to remove 3rd dimension of mask\n                    axs2[1, 1].set_title(\"Pred-Mask Without Threshold\", fontsize=30)\n                    \n                    plt.show()\n                    \n                    img_dir = os.path.join(\"/kaggle/working/\", f\"epoch{pl_module.current_epoch}_val_batch{batch_idx}_image{img_id}.png\")\n                    fig2.savefig(img_dir, bbox_inches='tight') \n                    wandb_logger.log_image(key=f\"val_batch{batch_idx}_image{img_id}\", images=[img_dir])\n                    \n                    # Save only first 4 images in the batch\n                    if img_id == pl_module.cfg.vis_img_num - 1:\n                        break\n                        \n    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):\n        \"\"\"Called when the train batch ends.\"\"\"\n        # outputs: The outputs of train_step_end(train_step(x))??\n        # Let's log all sample image predictions from the fifth batch\n        if batch_idx == pl_module.cfg.vis_val_batch_id: \n            with torch.no_grad():   \n                # x, y = batch[\"image\"], batch[\"mask\"]\n                x, y, y_distance, y_distance_sum = batch[\"image\"], batch[\"mask\"], batch[\"distance_mask\"], batch[\"distance_mask_sum\"]\n                \n                # All five images and preds in one figure\n                fig1, axs1 = plt.subplots(pl_module.cfg.vis_img_num, 5, figsize=(50, 50)) # pl_module.cfg.val_dl.batch_size\n                for img_id, (img_gt, mask_gt, mask_dist_gt, mask_pred) in enumerate(zip(x, y, y_distance, outputs[\"prob_y\"])): # [1, num_channels, height, width]\n                    mask_pred_th = (mask_pred > 0.5).float()\n                    un_img = img_gt.cpu().numpy().squeeze().transpose(1, 2, 0) # for visualization we have to transpose back to HWC\n                    img_gt = (pl_module.unnorm_transform(image=un_img)[\"image\"]*255).to(torch.uint8)\n                    \n                    axs1[img_id, 0].imshow(np.asarray(visF.to_pil_image(img_gt)))\n                    axs1[img_id, 0].set_title(\"Image\", fontsize=30)\n\n                    axs1[img_id, 1].imshow(mask_dist_gt.cpu().numpy().squeeze())  # for visualization we have to remove 3rd dimension of mask\n                    axs1[img_id, 1].set_title(\"GT-DistanceMask\", fontsize=30)\n                    \n                    axs1[img_id, 2].imshow(mask_gt.cpu().numpy().squeeze())  # for visualization we have to remove 3rd dimension of mask\n                    axs1[img_id, 2].set_title(\"GT-Mask\", fontsize=30)\n    \n                    axs1[img_id, 3].imshow(mask_pred.cpu().numpy().squeeze())  # for visualization we have to remove 3rd dimension of mask\n                    axs1[img_id, 3].set_title(\"Pred-Mask\", fontsize=30)\n                        \n                    axs1[img_id, 4].imshow(mask_pred_th.cpu().numpy().squeeze())  # for visualization we have to remove 3rd dimension of mask\n                    axs1[img_id, 4].set_title(\"Pred-Mask Thresholded\", fontsize=30)\n                    \n                    # Save only first 4 images in the batch\n                    if img_id == pl_module.cfg.vis_img_num - 1:\n                        break\n                plt.show()\n                img_dir = os.path.join(\"/kaggle/working/\", f\"train_epoch{pl_module.current_epoch}_data_pred_vis.png\")\n                fig1.savefig(img_dir, bbox_inches='tight') \n                wandb_logger.log_image(key=f\"train_batch{batch_idx}_all\", images=[img_dir])\n                \n                # Four images and their preds are in different figures\n                fig2, axs2 = plt.subplots(2, 2, figsize=(30, 30)) \n                for img_id, (img_gt, mask_gt, mask_pred) in enumerate(zip(x, y, outputs[\"prob_y\"])): # [1, num_channels, height, width]\n                    mask_pred_th = (mask_pred > 0.5).squeeze(0)\n                    mask_gt_th = mask_gt > 0.5 # boolean\n                    \n                    un_img = img_gt.cpu().numpy().squeeze().transpose(1, 2, 0) # for visualization we have to transpose back to HWC\n                    img = (pl_module.unnorm_transform(image=un_img)[\"image\"]*255).to(torch.uint8)\n                         \n                    gt_mask_on_img = draw_segmentation_masks(image=img, masks=mask_gt_th, alpha=0.7, colors=[\"orange\"]) # Tensor[C, H, W]\n                    pred_mask_on_img = draw_segmentation_masks(image=img, masks=mask_pred_th, alpha=0.7, colors=[\"orange\"]) # Tensor[C, H, W]\n                    \n                    axs2[0, 0].imshow(np.asarray(visF.to_pil_image(gt_mask_on_img)))\n                    axs2[0, 0].set_title(\"GT-Mask on Image\", fontsize=30)\n                    \n                    axs2[0, 1].imshow(np.asarray(visF.to_pil_image(pred_mask_on_img)))\n                    axs2[0, 1].set_title(\"Pred-Mask on Image\", fontsize=30)   \n        \n                    axs2[1, 0].imshow(mask_gt.cpu().numpy().squeeze())  # for visualization we have to remove 3rd dimension of mask\n                    axs2[1, 0].set_title(\"GT-Mask\", fontsize=30)\n                        \n                    axs2[1, 1].imshow(mask_pred.cpu().numpy().squeeze())  # for visualization we have to remove 3rd dimension of mask\n                    axs2[1, 1].set_title(\"Pred-Mask Without Threshold\", fontsize=30)\n                    \n                    plt.show()\n                    \n                    img_dir = os.path.join(\"/kaggle/working/\", f\"epoch{pl_module.current_epoch}_train_batch{batch_idx}_image{img_id}.png\")\n                    fig2.savefig(img_dir, bbox_inches='tight') \n                    wandb_logger.log_image(key=f\"train_batch{batch_idx}_image{img_id}\", images=[img_dir])\n                    \n                    # Save only first 4 images in the batch\n                    if img_id == pl_module.cfg.vis_img_num - 1:\n                        break","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:03:38.654246Z","iopub.execute_input":"2023-07-24T13:03:38.654983Z","iopub.status.idle":"2023-07-24T13:03:38.709523Z","shell.execute_reply.started":"2023-07-24T13:03:38.654935Z","shell.execute_reply":"2023-07-24T13:03:38.708390Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# WANDB SEGMENTASYON GÖRSELLEŞTİRMELERİ PEK İYİ DEĞİL\n# https://docs.wandb.ai/guides/track/log/media#image-overlays\n# to control when you log to Weights & Biases via the WandbLogger\nclass LogSegPredOverlayCallback(Callback):\n    def on_validation_batch_end(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx):\n        \"\"\"Called when the validation batch ends.\"\"\"\n        # outputs: The outputs of validation_step_end(validation_step(x))\n        # Let's log all sample image predictions from the fifth batch\n        if batch_idx == pl_module.cfg.vis_val_batch_id:\n            # self.eval()\n            with torch.no_grad():   \n                x, y = batch[\"image\"], batch[\"mask\"]\n                images = [img for img in x]\n                # for visualization in wandb, masks must be in numpy format without 3rd dimension\n                masks = [{\"predictions\": {\"mask_data\": (mask_pred > 0.5).float().cpu().detach().numpy().squeeze(), \"class_labels\": {1: \"car\"}},\n                          \"ground_truth\": {\"mask_data\": mask_gt.cpu().detach().numpy().squeeze(), \"class_labels\": {1: \"car\"}}}\n                          for mask_gt, mask_pred in zip(y, outputs[\"prob_y\"])]\n            wandb_logger.log_image(\n                key=f\"val_batch{batch_idx}_image{img_id}\", \n                images=images, \n                masks=masks)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:03:38.989996Z","iopub.execute_input":"2023-07-24T13:03:38.990352Z","iopub.status.idle":"2023-07-24T13:03:39.001772Z","shell.execute_reply.started":"2023-07-24T13:03:38.990319Z","shell.execute_reply":"2023-07-24T13:03:39.000691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ImageSegModel(cfg, train_transform, val_transform, test_transform, unnorm_transform)\nModelSummary(model) #to see detailed layer based parameter nums max_depth=-1","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:03:39.233034Z","iopub.execute_input":"2023-07-24T13:03:39.233767Z","iopub.status.idle":"2023-07-24T13:03:40.991234Z","shell.execute_reply.started":"2023-07-24T13:03:39.233729Z","shell.execute_reply":"2023-07-24T13:03:40.989874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb_logger = WandbLogger(\n    project=cfg.wandb.proj_name,\n    name=f\"{cfg.model.model_name}/{cfg.model.encoder_name}/{cfg.wandb.exp_name}\",\n    group= f\"{cfg.model.model_name}/{cfg.model.encoder_name}/{cfg.data_name}\",\n    log_model=\"all\", # model checkpoints are logged during training\n)\nwandb_logger.watch(model, log=\"all\") # log and monitor gradients, parameter histogram and model topology as we train  \n# W&B summary metric to display the min, max, mean or best value for that metric\nwandb.define_metric('train_per_image_dice', summary='max')\nwandb.define_metric('val_per_image_dice', summary='max')\nwandb.define_metric('train_per_image_iou', summary='max')\nwandb.define_metric('val_per_image_iou', summary='max')\nwandb.define_metric('train_per_image_bIoU', summary='max')\nwandb.define_metric('val_per_image_bIoU', summary='max')\nwandb.define_metric('train_distance_transform_evalmetric', summary='max')\nwandb.define_metric('val_distance_transform_evalmetric', summary='max')\nwandb.define_metric('train_loss', summary='min')\nwandb.define_metric('val_loss', summary='min')\n\nmodel_checkpointer = ModelCheckpoint(\n    monitor=cfg.model_ckpt_motior,\n    mode=\"max\", # log model only if `val_per_image_iou` increases\n    save_top_k=1, # to save the best model. save_top_k=-1 to save all models\n    # every_n_epochs=5, # to save at every n epochs\n    save_last=True,\n    # To save locally:\n    dirpath=cfg.ckpt_save_dir,\n    filename='{epoch}-'+f'{cfg.model.model_name}-{cfg.model.encoder_name}-lr{cfg.optimizer.lr}-hight{cfg.transform.image_resize_h}-width{cfg.transform.image_resize_w}-{cfg.data_name}'\n)\n\nearlystop_checkpointer = EarlyStopping(\n    monitor=\"val_loss\", mode=\"min\", patience=cfg.patience, verbose=True\n) # verbose = 0, means silent.\n\nlr_monitor = LearningRateMonitor() # logging_interval='epoch'/'step'. Set to None to log at individual interval according to the interval key of each scheduler","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:03:40.994233Z","iopub.execute_input":"2023-07-24T13:03:40.995195Z","iopub.status.idle":"2023-07-24T13:04:12.235519Z","shell.execute_reply.started":"2023-07-24T13:03:40.995150Z","shell.execute_reply":"2023-07-24T13:04:12.234555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = pl.Trainer(\n    max_epochs=cfg.max_epoch,\n    max_time=cfg.max_time, # Stop after max_time hours of training or when reaching max_epochs epochs\n    precision=cfg.trainer.precision, # Mixed Precision (16-bit) combines the use of both 32 and 16-bit floating points to reduce memory footprint\n    accelerator=cfg.trainer.accelerator, # gpu, tpu, auto\n    devices=cfg.trainer.device_num,\n    callbacks=[TQDMProgressBar(refresh_rate=20), # for notebook usage\n               # earlystop_checkpointer,\n               model_checkpointer,\n               LogSegPredictionCallback(),\n               lr_monitor,\n               # ReduceLROnPlateauOptCallback()\n              ],\n    logger=[CSVLogger(save_dir=\"logs/\"), wandb_logger], # multiple loggers\n    strategy=None if cfg.trainer.device_num==1 else 'dp', # 'dp' strategy is used when multiple-gpus with 1 machine.\n    deterministic=True, # for reproducibity\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:04:12.237063Z","iopub.execute_input":"2023-07-24T13:04:12.237703Z","iopub.status.idle":"2023-07-24T13:04:13.967785Z","shell.execute_reply.started":"2023-07-24T13:04:12.237659Z","shell.execute_reply":"2023-07-24T13:04:13.966480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.fit(model) # ckpt_path=cfg.trainer.ckpt_path4resume, ckpt_path=\"best\"","metadata":{"execution":{"iopub.status.busy":"2023-07-24T13:04:13.974836Z","iopub.execute_input":"2023-07-24T13:04:13.983779Z","iopub.status.idle":"2023-07-24T13:05:39.221916Z","shell.execute_reply.started":"2023-07-24T13:04:13.983720Z","shell.execute_reply":"2023-07-24T13:05:39.216575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.test(model, ckpt_path=\"best\") # ckpt_path=\"last\" to load and evaluate the last model\n# EXPERIMENT 1\n# unet with resnet34 encoder\n    # 1 T4 GPU: 8.9 dk + 8.50 dk-> 2 T4 GPU: 7.22 dk + 6.48 dk\n\n# EXPERIMENT 2\n# unet with resnet34 encoder\n    # 2 T4 GPU & num_workers=0: 7.38 dk + 7.2 dk -> 2 T4 GPU & num_workers=2: 6.31 dk + 6.32 dk ---\n    # ---> 2 T4 GPU & num_workers=2 & mixed precision & 512x512: 4.1 dk + 4.1 dk\n    \n# EXPERIMENT 3\n# unet with resnet34 encoder\n    # 2 T4 GPU & num_workers=2 & mixed precision & 512x512 & wandb & pin_memory: 4.20 dk + 4.20 dk ---\n    # ---> 1 P100 GPU & num_workers=2 & mixed precision & 512x512 & wandb & pin_memory: 7.03 dk + 7.03 dk\n    \n# EXPERIMENT 4\n# unet with resnet34 encoder\n    # 2 T4 GPU & num_workers=2 & mixed precision & 512x512 & wandb & pin_memory & batch size 4: 4.28 dk + 4.28 dk ---\n    # ---> 2 T4 GPU & num_workers=2 & mixed precision & 512x512 & wandb & pin_memory & batch size 16: 2.39 dk + 3.01 dk + 2.57 dk\n    # ---> 2 T4 GPU & num_workers=2 & mixed precision & 512x512 & wandb & pin_memory & batch size 64: 2.47 dk + 2.46 dk\n    # ---> 2 T4 GPU & num_workers=2 & mixed precision & 512x512 & wandb & pin_memory & batch size 128: NOT FITTING INTO VRAM\n    \n# EXPERIMENT 5\n# unet with resnet34 encoder\n    # 2 T4 GPU & num_workers=2 & mixed precision & 512x512 & wandb & pin_memory & batch size 64 & dice and bce loss: 2.47 dk + 2.46 dk\n    # 2 T4 GPU & num_workers=2 & mixed precision & 512x512 & wandb & pin_memory & batch size 64 & distance loss: 3.34 dk + 3.39 dk","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LOAD MODEL FROM CKPT","metadata":{}},{"cell_type":"code","source":"model4eval = model # ImageSegModel.load_from_checkpoint(\"/kaggle/input/unet-sil/model_deneme\")\n\n# disable randomness, dropout, etc...\nmodel4eval.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_ds = CocoToSmpDataset(root=os.path.join(cfg.data_dir, \"test\"), \n                                annFile=os.path.join(cfg.data_dir, \"annotations_test.json\"),\n                                transforms=test_transform\n                          )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show(imgs):\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=(50, 50))\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = visF.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n        return fig, axs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"un_img = np.array(test_ds[4][\"image\"]).transpose((1, 2, 0)) # CHW -> HWC\nimg = (unnorm_transform(image=un_img)[\"image\"]*255).to(torch.uint8)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_gt = torch.tensor(test_ds[4][\"mask\"]) > 0.5 # boolean\nshow(draw_segmentation_masks(image=img, masks=mask_gt, alpha=0.5, colors=[\"orange\"]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predict with the model\nun_img = test_ds[4][\"image\"].unsqueeze(0)\ny_hat = model4eval(un_img)\nmask_pred = y_hat.sigmoid()\nmask_pred = (mask_pred > 0.5).squeeze(0)\nshow(draw_segmentation_masks(image=img, masks=mask_pred, alpha=0.5, colors=[\"orange\"]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LOAD MODEL FROM WANDB CKPT","metadata":{}},{"cell_type":"code","source":"# run = wandb.init(project=\"foreground-car-segm\")\n\n# reference can be retrieved in artifacts panel\n# \"VERSION\" can be a version (ex: \"v2\") or an alias (\"latest or \"best\")\n# checkpoint_reference = \"frkangul/foreground-car-segm/model-ugk2wlcc:v7\"\n\n# download checkpoint locally (if not already cached)\n# artifact = run.use_artifact(checkpoint_reference, type=\"model\")\n# artifact_dir = artifact.download()\n\n# load checkpoint\n# model4eval = ImageSegModel.load_from_checkpoint(Path(artifact_dir) / \"model.ckpt\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# trainer.test(model4eval)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}